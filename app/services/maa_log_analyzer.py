import json
import os
import re
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
from loguru import logger

from app.core import Config


def analyze_maa_logs(logs_directory: Path):
    """
    éå† logs_directory ä¸‹æ‰€æœ‰ .log æ–‡ä»¶ï¼Œè§£æå…¬æ‹›å’Œæ‰è½ä¿¡æ¯ï¼Œå¹¶ä¿å­˜ä¸º JSON æ–‡ä»¶
    """
    if not logs_directory.exists():
        logger.error(f"ç›®å½•ä¸å­˜åœ¨: {logs_directory}")
        return

    # **æ£€æŸ¥å¹¶åˆ é™¤è¶…æœŸæ—¥å¿—**
    clean_old_logs(logs_directory)

    # è®¾å®š JSON è¾“å‡ºè·¯å¾„
    json_output_path = logs_directory / f"{logs_directory.name}.json" if logs_directory.parent.name == "maa_run_history" else logs_directory.parent / f"{logs_directory.name}.json"

    aggregated_data = {
        # "recruit_statistics": defaultdict(int),
        "drop_statistics": defaultdict(lambda: defaultdict(int)),
    }

    log_files = list(logs_directory.rglob("*.log"))
    if not log_files:
        logger.error(f"æ²¡æœ‰æ‰¾åˆ° .log æ–‡ä»¶: {logs_directory}")
        return

    for log_file in log_files:
        analyze_single_log(log_file, aggregated_data)

    # ç”Ÿæˆ JSON æ–‡ä»¶
    with open(json_output_path, "w", encoding="utf-8") as json_file:
        json.dump(aggregated_data, json_file, ensure_ascii=False, indent=4)

    logger.info(f"ç»Ÿè®¡å®Œæˆï¼š{json_output_path}")

def analyze_single_log(log_file_path: Path, aggregated_data):
    """
    è§£æå•ä¸ª .log æ–‡ä»¶ï¼Œæå–å…¬æ‹›ç»“æœ & å…³å¡æ‰è½æ•°æ®
    """
    # recruit_data = aggregated_data["recruit_statistics"]
    drop_data = aggregated_data["drop_statistics"]

    with open(log_file_path, "r", encoding="utf-8") as f:
        logs = f.readlines()

    # # **å…¬æ‹›ç»Ÿè®¡**
    # i = 0
    # while i < len(logs):
    #     if "å…¬æ‹›è¯†åˆ«ç»“æœ:" in logs[i]:
    #         tags = []
    #         i += 1
    #         while i < len(logs) and "Tags" not in logs[i]:  # è¯»å–æ‰€æœ‰å…¬æ‹›æ ‡ç­¾
    #             tags.append(logs[i].strip())
    #             i += 1
    #
    #         if i < len(logs) and "Tags" in logs[i]:  # ç¡®ä¿ Tags è¡Œå­˜åœ¨
    #             star_match = re.search(r"(\d+)\s*Tags", logs[i])  # æå– 3,4,5,6 æ˜Ÿ
    #             if star_match:
    #                 star_level = f"{star_match.group(1)}â˜…"
    #                 recruit_data[star_level] += 1
    #     i += 1

    # **æ‰è½ç»Ÿè®¡**
    current_stage = None
    for i, line in enumerate(logs):
        drop_match = re.search(r"(\d+-\d+) æ‰è½ç»Ÿè®¡:", line)
        if drop_match:
            current_stage = drop_match.group(1)
            continue

        if current_stage and re.search(r"(\S+)\s*:\s*(\d+)\s*\(\+\d+\)", line):
            item_match = re.findall(r"(\S+)\s*:\s*(\d+)\s*\(\+(\d+)\)", line)
            for item, total, increment in item_match:
                drop_data[current_stage][item] += int(increment)

    logger.info(f"å¤„ç†å®Œæˆï¼š{log_file_path}")


def clean_old_logs(logs_directory: Path):
    """
    åˆ é™¤è¶…è¿‡ç”¨æˆ·è®¾å®šå¤©æ•°çš„æ—¥å¿—æ–‡ä»¶
    """
    retention_setting = Config.global_config.get(Config.global_config.function_LogRetentionDays)
    retention_days_mapping = {
        "7 å¤©": 7,
        "15 å¤©": 15,
        "30 å¤©": 30,
        "60 å¤©": 60,
        "æ°¸ä¸æ¸…ç†": None
    }

    retention_days = retention_days_mapping.get(retention_setting, None)
    if retention_days is None:
        logger.info("ğŸ”µ ç”¨æˆ·è®¾ç½®æ—¥å¿—ä¿ç•™æ—¶é—´ä¸ºã€æ°¸ä¸æ¸…ç†ã€‘ï¼Œè·³è¿‡æ¸…ç†")
        return

    cutoff_time = datetime.now() - timedelta(days=retention_days)

    deleted_count = 0
    for log_file in logs_directory.rglob("*.log"):
        file_time = datetime.fromtimestamp(log_file.stat().st_mtime)  # è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        if file_time < cutoff_time:
            try:
                os.remove(log_file)
                deleted_count += 1
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤è¶…æœŸæ—¥å¿—: {log_file}")
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æ—¥å¿—å¤±è´¥: {log_file}, é”™è¯¯: {e}")

    logger.info(f"âœ… æ¸…ç†å®Œæˆ: {deleted_count} ä¸ªæ—¥å¿—æ–‡ä»¶")

# # è¿è¡Œä»£ç 
# logs_directory = Path("")
# analyze_maa_logs(logs_directory)
